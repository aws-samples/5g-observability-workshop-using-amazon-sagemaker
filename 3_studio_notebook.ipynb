{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed086066-7a2c-4be4-9e95-5192e7b0d5b3",
   "metadata": {},
   "source": [
    "## Module 3: Training Custom Models Using Studio Notebook\n",
    "\n",
    "In many situations, you may choose to build a custom model when you need to tackle a unique problem or when there isn't a pre-built model that meets your needs. In such cases, building a custom model might involve selecting an appropriate algorithm, fine-tuning its parameters, and optimizing its performance through iterative experimentation. In this module we will going through following steps to build, track, deploy and monitor a custom model using Amazon SageMaker Studio Notebook.\n",
    "\n",
    "- [Step 1: Pull Data from Offline Feature Store](#Pull-data-from-offline-feature-store)\n",
    "- [Step 2: Train, Track, and Deploy a Xgboost Model](#Train-XGBoost-Model)\n",
    "- [Step 3: Train, Track, and Deploy an Isolation Forest Model](#Train-Isolation-Forest-Model)\n",
    "- [Step 4: Model Monitoring](#Model-Monitoring)\n",
    "- [Step 5: Clean Up](#Clean-up)\n",
    "\n",
    "**If you DID NOT run the previous modules, please run [0_setup.ipynb notebook](0_setup.ipynb) first before running this notebook**\n",
    "\n",
    "**This Demo is optimized for SageMaker Studio using Studio notebook in Data Science Kernel**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e7f468-dbfb-4b5b-bc97-11cf9043fca3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install required and/or update libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df25350d-9eb7-45dd-b594-b0020d93ebfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip --quiet\n",
    "\n",
    "!pip install -Uq awswrangler sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da76413c-8691-428a-ae8c-5b7097b26e9e",
   "metadata": {},
   "source": [
    "### Import & Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3372368e-1e64-4f10-867d-e76d403ed02c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'region' (str)\n",
      "Stored 'bucket' (str)\n",
      "Stored 'sagemaker_role' (str)\n",
      "Stored 'prefix' (str)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "prefix = \"telco-5g-observabiltiy\"\n",
    "\n",
    "%store region\n",
    "%store bucket\n",
    "%store sagemaker_role\n",
    "%store prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39b797-117b-45db-8f48-d181c2f51874",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pull data from offline feature store\n",
    "----\n",
    "In Module 1 of this workshop, we prepared the raw data and upload the final data into an Offline Feature Store. This dataset is now cataloged in a central location for management and discovery. Now we want to extract that data and build our observability models. SageMaker feature store uses athena query to pull the data and can cast the data directly into a pandas dataframe for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2eb1aed-2bcb-4251-a773-688951a59de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Query 30c12778-8193-4048-b179-cd87274b3c8c is being executed.\n",
      "INFO:sagemaker:Query 30c12778-8193-4048-b179-cd87274b3c8c successfully executed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>health</th>\n",
       "      <th>accessibility</th>\n",
       "      <th>5g_users</th>\n",
       "      <th>contention_rate</th>\n",
       "      <th>utilization</th>\n",
       "      <th>downlink_throughput</th>\n",
       "      <th>uplink_throughput</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>location_id</th>\n",
       "      <th>eventtime</th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.112072</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.263812</td>\n",
       "      <td>0.044078</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>1</td>\n",
       "      <td>18EIGHTYR_401</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:38.569</td>\n",
       "      <td>2023-03-25 19:23:40.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151934</td>\n",
       "      <td>0.009691</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0</td>\n",
       "      <td>AGUSTINMALR_401_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:38.569</td>\n",
       "      <td>2023-03-25 19:23:40.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.109190</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.360497</td>\n",
       "      <td>0.047669</td>\n",
       "      <td>0.096913</td>\n",
       "      <td>0</td>\n",
       "      <td>18EIGHTYR_401</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:38.569</td>\n",
       "      <td>2023-03-25 19:23:40.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0</td>\n",
       "      <td>AMPAROVILCALN_401_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:38.569</td>\n",
       "      <td>2023-03-25 19:23:41.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.92</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156077</td>\n",
       "      <td>0.006322</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0</td>\n",
       "      <td>AGUSTINMALR_401_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:38.569</td>\n",
       "      <td>2023-03-25 19:23:41.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84564</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.018252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160221</td>\n",
       "      <td>0.015909</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0</td>\n",
       "      <td>BUGALLONMKNAR_401_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:50.319</td>\n",
       "      <td>2023-03-25 19:28:36.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84565</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.125520</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.393646</td>\n",
       "      <td>0.102649</td>\n",
       "      <td>0.134556</td>\n",
       "      <td>0</td>\n",
       "      <td>BOLANOS2M_353_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:50.319</td>\n",
       "      <td>2023-03-25 19:28:36.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84566</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.028178</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.296961</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.013804</td>\n",
       "      <td>0</td>\n",
       "      <td>BULACANWAWAPILILARZLN-403_4RFS_None</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:50.319</td>\n",
       "      <td>2023-03-25 19:28:36.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84567</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.011207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147790</td>\n",
       "      <td>0.003988</td>\n",
       "      <td>0.010599</td>\n",
       "      <td>0</td>\n",
       "      <td>BUNGADQCR_402_4RFS</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:50.319</td>\n",
       "      <td>2023-03-25 19:28:36.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84568</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.025296</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.291436</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>0.013757</td>\n",
       "      <td>0</td>\n",
       "      <td>ASANTOSISIDROPQUENCRN-401_4RFS_None</td>\n",
       "      <td>1.679772e+09</td>\n",
       "      <td>2023-03-25 19:28:50.319</td>\n",
       "      <td>2023-03-25 19:28:37.000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84569 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       health  accessibility  5g_users  contention_rate  utilization  \\\n",
       "0        1.00           1.00  0.112072         0.001814     0.263812   \n",
       "1        0.96           1.00  0.014729         0.000000     0.151934   \n",
       "2        1.00           0.99  0.109190         0.000680     0.360497   \n",
       "3        1.00           1.00  0.001281         0.000000     0.276243   \n",
       "4        0.92           1.00  0.019212         0.000000     0.156077   \n",
       "...       ...            ...       ...              ...          ...   \n",
       "84564    0.99           0.98  0.018252         0.000000     0.160221   \n",
       "84565    1.00           1.00  0.125520         0.002948     0.393646   \n",
       "84566    1.00           0.99  0.028178         0.000227     0.296961   \n",
       "84567    1.00           1.00  0.011207         0.000000     0.147790   \n",
       "84568    0.99           0.98  0.025296         0.000680     0.291436   \n",
       "\n",
       "       downlink_throughput  uplink_throughput  anomaly  \\\n",
       "0                 0.044078           0.042050        1   \n",
       "1                 0.009691           0.003334        0   \n",
       "2                 0.047669           0.096913        0   \n",
       "3                 0.000590           0.000726        0   \n",
       "4                 0.006322           0.004083        0   \n",
       "...                    ...                ...      ...   \n",
       "84564             0.015909           0.006789        0   \n",
       "84565             0.102649           0.134556        0   \n",
       "84566             0.007017           0.013804        0   \n",
       "84567             0.003988           0.010599        0   \n",
       "84568             0.002612           0.013757        0   \n",
       "\n",
       "                               location_id     eventtime  \\\n",
       "0                            18EIGHTYR_401  1.679772e+09   \n",
       "1                     AGUSTINMALR_401_4RFS  1.679772e+09   \n",
       "2                            18EIGHTYR_401  1.679772e+09   \n",
       "3                   AMPAROVILCALN_401_4RFS  1.679772e+09   \n",
       "4                     AGUSTINMALR_401_4RFS  1.679772e+09   \n",
       "...                                    ...           ...   \n",
       "84564               BUGALLONMKNAR_401_4RFS  1.679772e+09   \n",
       "84565                   BOLANOS2M_353_4RFS  1.679772e+09   \n",
       "84566  BULACANWAWAPILILARZLN-403_4RFS_None  1.679772e+09   \n",
       "84567                   BUNGADQCR_402_4RFS  1.679772e+09   \n",
       "84568  ASANTOSISIDROPQUENCRN-401_4RFS_None  1.679772e+09   \n",
       "\n",
       "                    write_time      api_invocation_time  is_deleted  \n",
       "0      2023-03-25 19:28:38.569  2023-03-25 19:23:40.000       False  \n",
       "1      2023-03-25 19:28:38.569  2023-03-25 19:23:40.000       False  \n",
       "2      2023-03-25 19:28:38.569  2023-03-25 19:23:40.000       False  \n",
       "3      2023-03-25 19:28:38.569  2023-03-25 19:23:41.000       False  \n",
       "4      2023-03-25 19:28:38.569  2023-03-25 19:23:41.000       False  \n",
       "...                        ...                      ...         ...  \n",
       "84564  2023-03-25 19:28:50.319  2023-03-25 19:28:36.000       False  \n",
       "84565  2023-03-25 19:28:50.319  2023-03-25 19:28:36.000       False  \n",
       "84566  2023-03-25 19:28:50.319  2023-03-25 19:28:36.000       False  \n",
       "84567  2023-03-25 19:28:50.319  2023-03-25 19:28:36.000       False  \n",
       "84568  2023-03-25 19:28:50.319  2023-03-25 19:28:37.000       False  \n",
       "\n",
       "[84569 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "%store -r fg_name\n",
    "\n",
    "anomaly_features = FeatureGroup(name=fg_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "query = anomaly_features.athena_query()\n",
    "\n",
    "table_name = query.table_name\n",
    "                       \n",
    "query_string = f\"\"\"\n",
    "SELECT * FROM \"{table_name}\"\n",
    "\"\"\"\n",
    "\n",
    "query.run(query_string=query_string, output_location=f\"s3://{bucket}/{prefix}/data/query_results\")\n",
    "query.wait()\n",
    "\n",
    "dataset = query.as_dataframe()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12085adb-bf4e-41b9-aca2-c361af728fd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train XGBoost Model\n",
    "----\n",
    "\n",
    "In real world, data scientist goes through hundreds of iterations to experiment with different algorithm to come up with the best model for the ML use case. Here you are going to start with a supervised learning approach and use XGboost model for our problem.\n",
    "\n",
    "To get Your features ready for XGBoost, we need to move the target varibale to the first column for our xgboost model. You will also split the data into train & test dataset to keep a holdout set to validate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "efdf60e5-b1da-4b2e-bd19-330ee48d7bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_order = [\"anomaly\"] + list(dataset.drop([\"location_id\", \"anomaly\", \"eventtime\", \"write_time\",\"api_invocation_time\",'is_deleted'], axis=1).columns)\n",
    "\n",
    "train = dataset.sample(frac=0.80, random_state=0)[col_order]\n",
    "test = dataset.drop(train.index)[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e213e0-8814-4a3d-bebe-42bce9e5ef1e",
   "metadata": {},
   "source": [
    "Upload the training data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01ae425b-8b66-497c-b7db-3875476f5698",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is uploaded to s3://sagemaker-us-west-2-376678947624/telco-5g-observabiltiy/data/xgboost/train.csv\n"
     ]
    }
   ],
   "source": [
    "train.to_csv(\"data/train.csv\", index=False)\n",
    "key = f\"{prefix}/data/xgboost/train.csv\"\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/train.csv\",\n",
    "    Bucket=bucket,\n",
    "    Key=key,\n",
    ")\n",
    "\n",
    "train_s3_path = f\"s3://{bucket}/{key}\"\n",
    "print(f\"training data is uploaded to {train_s3_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4591aa-c85a-45e9-9815-3447503e543b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set the hyperparameters\n",
    "These are the parameters which will be sent to our training script in order to train the model. Although they are all defined as \"hyperparameters\" here, they can encompass XGBoost's [Learning Task Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters), [Tree Booster Parameters](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster), or any other parameters you'd like to configure for XGBoost.\n",
    "\n",
    "#### Setup Experiment Run Context\n",
    "Amazon SageMaker Experiment allows data you to organize, track, compare, and evaluate experiments during the model building and training process. Experiment tracking is extremely important because it enables you to keep track of model performance and changes over time, making it easier to debug and optimize the model. It also helps in reproducing and sharing the results with others, leading to better collaboration and faster iteration. For more details reference [SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html).\n",
    "\n",
    "### Train a custom model on SageMaker\n",
    "When it comes to training a model on SageMaker, you start by specifying the type of instance, the framework container, and any hyperparameters you want to use. When you call `estimator.fit()`, you supply the location of your training data. SageMaker will then spin up the specified instance and download your training data onto it. In the example below, we are also supplying a custom training script. This way, SageMaker will copy the script into the container and run. This makes it easy for you to iterate your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "048f2863-a3d3-4949-844e-bae48e824473",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-03-25-19-30-48-305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-25 19:30:51 Starting - Starting the training job...\n",
      "2023-03-25 19:31:05 Starting - Preparing the instances for training...\n",
      "2023-03-25 19:31:50 Downloading - Downloading input data...\n",
      "2023-03-25 19:32:20 Training - Downloading the training image...\n",
      "2023-03-25 19:32:40 Training - Training image download completed. Training in progress.\u001b[34m[2023-03-25 19:32:48.425 ip-10-0-106-133.us-west-2.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-03-25 19:32:48.507 ip-10-0-106-133.us-west-2.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Module xgboost_starter_script does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:48:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\n",
      "  Downloading sagemaker-2.141.0.tar.gz (685 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 685.6/685.6 kB 16.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /miniconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.17.52)\u001b[0m\n",
      "\u001b[34mCollecting attrs<23,>=20.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 17.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting boto3\n",
      "  Downloading boto3-1.26.99-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 kB 26.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 16.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (3.20.1)\u001b[0m\n",
      "\u001b[34mCollecting protobuf3-to-dict<1.0,>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata<5.0,>=1.4.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.2.4)\u001b[0m\n",
      "\u001b[34mCollecting pathos\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 23.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.30.0,>=1.29.99\n",
      "  Downloading botocore-1.29.99-py3-none-any.whl (10.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 103.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /miniconda3/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.99->boto3->-r requirements.txt (line 2)) (1.26.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.99->boto3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /miniconda3/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker->-r requirements.txt (line 1)) (3.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /miniconda3/lib/python3.8/site-packages (from pandas->sagemaker->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting ppft>=1.7.6.6\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 14.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting dill>=0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 30.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pox>=0.3.2\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: xgboost-starter-script, sagemaker, protobuf3-to-dict\n",
      "  Building wheel for xgboost-starter-script (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for xgboost-starter-script (setup.py): finished with status 'done'\n",
      "  Created wheel for xgboost-starter-script: filename=xgboost_starter_script-1.0.0-py2.py3-none-any.whl size=8765 sha256=a25f62cfd5b8549738ee0a41a0e7e4a16e2784ca0a44ece1924e4829b51e4826\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-7r1x5r2c/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.141.0-py2.py3-none-any.whl size=927321 sha256=fb42fe92a242e5d156e8e5ac9ca1cce836c3cdf9165963c76d761538aa9dfac5\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/6d/01/7456ac4caa97b11be5ccc356e7e4599233e75629ad34ccd469\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4014 sha256=b6db166088d0921e8429cfcee28f00e966cd0f5d1d14ff286675477068babd9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\u001b[0m\n",
      "\u001b[34mSuccessfully built xgboost-starter-script sagemaker protobuf3-to-dict\u001b[0m\n",
      "\u001b[34mInstalling collected packages: xgboost-starter-script, smdebug_rulesconfig, protobuf3-to-dict, ppft, pox, importlib-metadata, google-pasta, dill, contextlib2, attrs, schema, multiprocess, botocore, s3transfer, pathos, boto3, sagemaker\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.52\n",
      "    Uninstalling botocore-1.20.52:\n",
      "      Successfully uninstalled botocore-1.20.52\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.3.7\n",
      "    Uninstalling s3transfer-0.3.7:\n",
      "      Successfully uninstalled s3transfer-0.3.7\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.52\n",
      "    Uninstalling boto3-1.17.52:\n",
      "      Successfully uninstalled boto3-1.17.52\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-containers 2.8.6.post2 requires typing, which is not installed.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires boto3==1.17.52, but you have boto3 1.26.99 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-xgboost-container 2.0 requires botocore==1.20.52, but you have botocore 1.29.99 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed attrs-22.2.0 boto3-1.26.99 botocore-1.29.99 contextlib2-21.6.0 dill-0.3.6 google-pasta-0.2.0 importlib-metadata-4.13.0 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf3-to-dict-0.1.5 s3transfer-0.6.0 sagemaker-2.141.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 xgboost-starter-script-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:57:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-03-25:19:32:57:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"max_depth\": \"3\",\n",
      "        \"num_round\": \"100\",\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"region\": \"us-west-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2023-03-25-19-30-48-305\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-376678947624/sagemaker-xgboost-2023-03-25-19-30-48-305/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"xgboost_starter_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"xgboost_starter_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"max_depth\":\"3\",\"num_round\":\"100\",\"objective\":\"binary:logistic\",\"region\":\"us-west-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=xgboost_starter_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=xgboost_starter_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-376678947624/sagemaker-xgboost-2023-03-25-19-30-48-305/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":\"0.2\",\"max_depth\":\"3\",\"num_round\":\"100\",\"objective\":\"binary:logistic\",\"region\":\"us-west-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2023-03-25-19-30-48-305\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-376678947624/sagemaker-xgboost-2023-03-25-19-30-48-305/source/sourcedir.tar.gz\",\"module_name\":\"xgboost_starter_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"xgboost_starter_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--max_depth\",\"3\",\"--num_round\",\"100\",\"--objective\",\"binary:logistic\",\"--region\",\"us-west-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=3\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=100\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=binary:logistic\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-west-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m xgboost_starter_script --eta 0.2 --max_depth 3 --num_round 100 --objective binary:logistic --region us-west-2\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (xgboost-experiment-1679772647-ca49) under experiment (telco-5g-observabiltiy-1679772647-d8f7) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34m[0]#011train-auc:0.9436684\u001b[0m\n",
      "\u001b[34m[1]#011train-auc std:0.0007316719483484425\u001b[0m\n",
      "\u001b[34m[2]#011validation-auc:0.9386445999999999\u001b[0m\n",
      "\u001b[34m[3]#011validation-auc std:0.002525982628602203\u001b[0m\n",
      "\u001b[34m[19:33:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\n",
      "2023-03-25 19:33:32 Uploading - Uploading generated training model\n",
      "2023-03-25 19:33:32 Completed - Training job completed\n",
      "Training seconds: 102\n",
      "Billable seconds: 102\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.experiments.run import Run, load_run\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "train_instance_count=1\n",
    "train_instance_type=\"ml.m5.xlarge\" \n",
    "\n",
    "experiment_name = unique_name_from_base(prefix)\n",
    "\n",
    "run_name = unique_name_from_base(\"xgboost-experiment\")\n",
    "\n",
    "with Run(experiment_name=experiment_name, run_name=run_name, \n",
    "         sagemaker_session=sagemaker_session) as run:\n",
    "        \n",
    "    run.log_file(\"data/train.csv\", is_output=False)\n",
    "    \n",
    "    hyperparameters = {\n",
    "        \"max_depth\": \"3\",\n",
    "        \"eta\": \"0.2\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": \"100\",\n",
    "        \"region\":region\n",
    "    }\n",
    "\n",
    "    xgb_estimator = XGBoost(\n",
    "        entry_point=\"xgboost_starter_script.py\",\n",
    "        source_dir=\"code\",\n",
    "        hyperparameters=hyperparameters,\n",
    "        role=sagemaker_role,\n",
    "        instance_count=train_instance_count,\n",
    "        instance_type=train_instance_type,        \n",
    "        framework_version=\"1.5-1\",\n",
    "    )\n",
    "    \n",
    "    xgb_estimator.fit(inputs={\"train\": train_s3_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b917f34-85d8-4666-b7ff-70921b86321a",
   "metadata": {},
   "source": [
    "#### Deploy model to an endpoint\n",
    "We are going to enable data capturing for model monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f65dff6e-e964-4396-a338-2955e2c125fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-03-25-19-33-45-597\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-03-25-19-33-45-597\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-03-25-19-33-45-597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f\"s3://{bucket}/{prefix}/monitoring/datacapture\"\n",
    ")\n",
    "\n",
    "\n",
    "predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.xlarge\", serializer=CSVSerializer(), data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e42e0c-95c0-4fc6-bf78-0f6f875496a9",
   "metadata": {},
   "source": [
    "#### Test inference on endpoint\n",
    "Function below calls the sagemaker endpoint and capture the predictions to generate the confussion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c221ae2-5c59-4156-8283-e42d53479aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions =[]\n",
    "    for array in split_array:\n",
    "        predictions = predictions + sum(predictor.predict(array), [])\n",
    "\n",
    "    return [float(i) for i in predictions]\n",
    "\n",
    "def calibrate(probabilities, cutoff=.2):\n",
    "    predictions = []\n",
    "    for p in probabilities:\n",
    "        if p <= cutoff:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23336fcb-b392-447c-9b3d-9311a91ec034",
   "metadata": {},
   "source": [
    "You can load SageMaker Experiment at any time and track more information about the run. Here you will invoke the SageMaker endpoint for batch prediction and we will put the results in a simple [confusion matrix](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/). We will also capture the chart and track it inside our experiment run.\n",
    "\n",
    "After you complete the cell below, you can go to SageMaker Experiment to see all the information SageMaker has captured for you in this experiment run.\n",
    "\n",
    "<img src=\"statics/module_03_ex01.png\"  width=\"75%\" height=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fee8c75f-d203-4467-bb90-a7f0cdbb0dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.experiments.run:run_name is explicitly supplied in load_run, which will be prioritized to load the Run object. In other words, the run name in the experiment config, fetched from the job environment or the current run context, will be ignored.\n",
      "INFO:sagemaker.experiments.run:The run (xgboost-experiment-1679772647-ca49) under experiment (telco-5g-observabiltiy-1679772647-d8f7) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: telco-5g-observabiltiy-1679772647-d8f7\n",
      "\n",
      "Run Name: xgboost-experiment-1679772647-ca49\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13585</td>\n",
       "      <td>774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>879</td>\n",
       "      <td>1676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions      0     1\n",
       "actual                  \n",
       "0            13585   774\n",
       "1              879  1676"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with load_run(experiment_name=experiment_name, run_name=run_name) as run:\n",
    "\n",
    "    # run batch prediction\n",
    "    probabilities = predict(test.to_numpy()[:, 1:])\n",
    "    # run calibration and visualize the results\n",
    "    predictions = np.asarray(calibrate(probabilities, 0.4))\n",
    "    run.log_confusion_matrix(test[\"anomaly\"], predictions, unique_name_from_base(\"Confusion-Matrix\"))\n",
    "\n",
    "print(f\"Experiment Name: {experiment_name}\\n\")\n",
    "\n",
    "print(f\"Run Name: {run_name}\\n\")\n",
    "\n",
    "pd.crosstab(\n",
    "    index=test.iloc[:, 0],\n",
    "    columns=predictions,\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe950a-d9f0-4dc1-b345-5a0b8ea41560",
   "metadata": {},
   "source": [
    "### Train Isolation Forest Model\n",
    "----\n",
    "Now let's experiment with an unsupervised approach. You will use the full dataset this time and try to build an isolation forest model to isolate anomalies base on how different they are from each other. More on [isolation forest algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f91a9-6aa0-4b59-a418-fdccc556e64b",
   "metadata": {},
   "source": [
    "Upload a different dataset to S3 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa7cdf22-b8fb-4a3a-82b3-4575cb37f9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data is uploaded to s3://sagemaker-us-west-2-376678947624/telco-5g-observabiltiy/data/isoforest/iso_input.csv\n"
     ]
    }
   ],
   "source": [
    "iso_input = dataset.drop([\"location_id\", \"anomaly\", \"eventtime\", \n",
    "                          \"write_time\",\"api_invocation_time\",'is_deleted'], axis=1)\n",
    "iso_input.to_csv(\"data/iso_input.csv\", index=False)\n",
    "key = f\"{prefix}/data/isoforest/iso_input.csv\"\n",
    "\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/iso_input.csv\",\n",
    "    Bucket=bucket,\n",
    "    Key=key,\n",
    ")\n",
    "\n",
    "input_s3_path = f\"s3://{bucket}/{key}\"\n",
    "print(f\"training data is uploaded to {input_s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d84fa7d-8122-422e-bd08-ef23cbab4f77",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2023-03-25-19-39-53-333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-25 19:39:54 Starting - Starting the training job...\n",
      "2023-03-25 19:40:09 Starting - Preparing the instances for training...\n",
      "2023-03-25 19:40:48 Downloading - Downloading input data...\n",
      "2023-03-25 19:41:33 Training - Training image download completed. Training in progress...\u001b[34m2023-03-25 19:41:41,100 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:41,103 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:41,111 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:41,292 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\n",
      "  Downloading sagemaker-2.141.0.tar.gz (685 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 685.6/685.6 kB 18.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /miniconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.24.17)\u001b[0m\n",
      "\u001b[34mCollecting attrs<23,>=20.3.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting boto3\n",
      "  Downloading boto3-1.26.99-py3-none-any.whl (135 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 135.5/135.5 kB 35.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-pasta\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (3.20.2)\u001b[0m\n",
      "\u001b[34mCollecting protobuf3-to-dict<1.0,>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting smdebug_rulesconfig==1.0.1\n",
      "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-metadata<5.0,>=1.4.0\n",
      "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=20.0\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.7/42.7 kB 13.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.8/site-packages (from sagemaker->-r requirements.txt (line 1)) (1.1.3)\u001b[0m\n",
      "\u001b[34mCollecting pathos\n",
      "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.8/79.8 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting schema\n",
      "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\u001b[0m\n",
      "\u001b[34mCollecting botocore<1.30.0,>=1.29.99\n",
      "  Downloading botocore-1.29.99-py3-none-any.whl (10.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.5/10.5 MB 122.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /miniconda3/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 2)) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /miniconda3/lib/python3.8/site-packages (from boto3->-r requirements.txt (line 2)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.99->boto3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.99->boto3->-r requirements.txt (line 2)) (1.26.14)\u001b[0m\n",
      "\u001b[34mCollecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker->-r requirements.txt (line 1)) (1.15.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /miniconda3/lib/python3.8/site-packages (from pandas->sagemaker->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mCollecting pox>=0.3.2\n",
      "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\u001b[0m\n",
      "\u001b[34mCollecting ppft>=1.7.6.6\n",
      "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 18.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multiprocess>=0.70.14\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.0/132.0 kB 38.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting dill>=0.3.6\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 33.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting contextlib2>=0.5.5\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker, protobuf3-to-dict\n",
      "  Building wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sagemaker (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker: filename=sagemaker-2.141.0-py2.py3-none-any.whl size=927321 sha256=b79d89aca292ad3d7b874529582bc6860c349412e5a99962074f07a64f378b85\n",
      "  Stored in directory: /root/.cache/pip/wheels/97/6d/01/7456ac4caa97b11be5ccc356e7e4599233e75629ad34ccd469\n",
      "  Building wheel for protobuf3-to-dict (setup.py): started\n",
      "  Building wheel for protobuf3-to-dict (setup.py): finished with status 'done'\n",
      "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4014 sha256=476412eea96dcd01bb1bd14eb563479ea8bd7f422dbe7ed471a418caeb763f38\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/10/27/2d1e23d8b9a9013a83fbb418a0b17b1e6f81c8db8f53b53934\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker protobuf3-to-dict\u001b[0m\n",
      "\u001b[34mInstalling collected packages: zipp, smdebug_rulesconfig, protobuf3-to-dict, ppft, pox, packaging, google-pasta, dill, contextlib2, attrs, schema, multiprocess, importlib-metadata, botocore, pathos, boto3, sagemaker\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.27.18\n",
      "    Uninstalling botocore-1.27.18:\n",
      "      Successfully uninstalled botocore-1.27.18\u001b[0m\n",
      "\u001b[34m  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.24.17\n",
      "    Uninstalling boto3-1.24.17:\n",
      "      Successfully uninstalled boto3-1.24.17\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires boto3==1.24.17, but you have boto3 1.26.99 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires botocore==1.27.18, but you have botocore 1.29.99 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed attrs-22.2.0 boto3-1.26.99 botocore-1.29.99 contextlib2-21.6.0 dill-0.3.6 google-pasta-0.2.0 importlib-metadata-4.13.0 multiprocess-0.70.14 packaging-23.0 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf3-to-dict-0.1.5 sagemaker-2.141.0 schema-0.7.5 smdebug_rulesconfig-1.0.1 zipp-3.15.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:48,339 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:48,350 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:48,362 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:48,370 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_samples\": 512,\n",
      "        \"random_state\": 42,\n",
      "        \"region\": \"us-west-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2023-03-25-19-39-53-333\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-376678947624/sagemaker-scikit-learn-2023-03-25-19-39-53-333/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"isolation_forest_script\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"isolation_forest_script.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max_samples\":512,\"random_state\":42,\"region\":\"us-west-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=isolation_forest_script.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=isolation_forest_script\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-376678947624/sagemaker-scikit-learn-2023-03-25-19-39-53-333/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_samples\":512,\"random_state\":42,\"region\":\"us-west-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2023-03-25-19-39-53-333\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-376678947624/sagemaker-scikit-learn-2023-03-25-19-39-53-333/source/sourcedir.tar.gz\",\"module_name\":\"isolation_forest_script\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"isolation_forest_script.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max_samples\",\"512\",\"--random_state\",\"42\",\"--region\",\"us-west-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SAMPLES=512\u001b[0m\n",
      "\u001b[34mSM_HP_RANDOM_STATE=42\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-west-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python isolation_forest_script.py --max_samples 512 --random_state 42 --region us-west-2\u001b[0m\n",
      "\n",
      "2023-03-25 19:41:59 Uploading - Uploading generated training model\u001b[34mINFO:sagemaker.experiments.run:The run (isoforest-experiment-1679773192-70a6) under experiment (telco-5g-observabiltiy-1679772647-d8f7) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mrequirements.txt\u001b[0m\n",
      "\u001b[34mxgboost_starter_script.py\u001b[0m\n",
      "\u001b[34misolation_forest_script.py\u001b[0m\n",
      "\u001b[34m#_anomalies = 10804; pct_anomalies = 12.775366860196998 %;\u001b[0m\n",
      "\u001b[34mavg_score = -0.40; std =  0.07;\u001b[0m\n",
      "\u001b[34m2023-03-25 19:41:54,856 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-03-25 19:42:09 Completed - Training job completed\n",
      "Training seconds: 82\n",
      "Billable seconds: 82\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "run_name = unique_name_from_base(\"isoforest-experiment\")\n",
    "\n",
    "with Run(experiment_name=experiment_name, run_name=run_name, \n",
    "         sagemaker_session=sagemaker_session) as run:\n",
    "    \n",
    "    run.log_file(\"data/iso_input.csv\", is_output=False)\n",
    "    FRAMEWORK_VERSION = \"1.0-1\"\n",
    "\n",
    "    sklearn = SKLearn(\n",
    "        entry_point=\"isolation_forest_script.py\",\n",
    "        source_dir=\"code\",\n",
    "        framework_version=\"1.0-1\",\n",
    "        instance_count=train_instance_count,\n",
    "        instance_type=train_instance_type,\n",
    "        role=sagemaker_role,\n",
    "        sagemaker_session=sagemaker_session,\n",
    "        hyperparameters={\"max_samples\": 512,\n",
    "                        \"random_state\": 42,\n",
    "                        \"region\":region},\n",
    "    )\n",
    "    sklearn.fit({\"train\": input_s3_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435e5bd-2cea-423b-824e-3b5c1e613f8a",
   "metadata": {},
   "source": [
    "#### Deploy IsoForest Model to an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf6581c0-2fb7-481e-a26f-53f818bf17b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-scikit-learn-2023-03-25-19-42-47-663\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-scikit-learn-2023-03-25-19-42-47-663\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-scikit-learn-2023-03-25-19-42-47-663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "isoforest_predictor = sklearn.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.xlarge\", serializer=CSVSerializer(), deserializer = JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e7ad6-d600-46cc-b39d-6f3cd8cb2428",
   "metadata": {},
   "source": [
    "#### Test inference on endpoint\n",
    "Capture the confussion matrix results in the experiemnt for historic reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7dcd43d9-5abe-4c70-95f3-ccaa6aebdb2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.experiments.run:run_name is explicitly supplied in load_run, which will be prioritized to load the Run object. In other words, the run name in the experiment config, fetched from the job environment or the current run context, will be ignored.\n",
      "INFO:sagemaker.experiments.run:The run (isoforest-experiment-1679773192-70a6) under experiment (telco-5g-observabiltiy-1679772647-d8f7) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name: telco-5g-observabiltiy-1679772647-d8f7\n",
      "\n",
      "Run Name: isoforest-experiment-1679773192-70a6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1541</td>\n",
       "      <td>12818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>659</td>\n",
       "      <td>1896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions     0      1\n",
       "actual                  \n",
       "0            1541  12818\n",
       "1             659   1896"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with load_run(experiment_name=experiment_name, run_name=run_name) as run:\n",
    "\n",
    "    results = isoforest_predictor.predict(test.to_numpy()[:, 1:])\n",
    "    \n",
    "    # run fix -1 value to 0\n",
    "    predictions = []\n",
    "    for x in results:\n",
    "        if x <= 0:\n",
    "            predictions.append(0)\n",
    "        else:\n",
    "            predictions.append(x)\n",
    "            \n",
    "    predictions = np.asarray(predictions)\n",
    "    run.log_confusion_matrix(test[\"anomaly\"], predictions, unique_name_from_base(\"IsoForest-Confusion-Matrix\"))\n",
    "\n",
    "print(f\"Experiment Name: {experiment_name}\\n\")\n",
    "\n",
    "print(f\"Run Name: {run_name}\\n\")\n",
    "\n",
    "pd.crosstab(\n",
    "    index=test.iloc[:, 0],\n",
    "    columns=predictions,\n",
    "    rownames=[\"actual\"],\n",
    "    colnames=[\"predictions\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5db1da-a43d-4679-ac49-c103b851e8c8",
   "metadata": {},
   "source": [
    "## Model Monitoring\n",
    "\n",
    "<img src=\"statics/module_03_monitor01.png\"  width=\"50%\" height=\"50%\">\n",
    "\n",
    "Model monitoring is crucial to ensure that machine learning models continue to perform as expected after deployment. It involves tracking various metrics such as accuracy, precision, recall, and F1 score to detect and diagnose performance degradation, identify data drift, and other issues that may arise over time.\n",
    "\n",
    "Amazon SageMaker model monitoring allows you to automatically monitor your deployed models using predefined rules, and alerts you when the model's performance deviates from the expected behavior. In this example you are going to manually setup data drift detection for our xgboost endpoint.\n",
    "\n",
    "#### 1. Create a baselining job with training dataset\n",
    "Now that you have the training data ready in Amazon S3, start a job to suggest constraints. DefaultModelMonitor.suggest_baseline(..) starts a ProcessingJob using an Amazon SageMaker provided Model Monitor container to generate the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "45fb2e50-6eea-4c0a-b293-99050dfa49ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: .\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2023-03-25-19-46-50-316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................\u001b[34m2023-03-25 19:51:04,337 - matplotlib.font_manager - INFO - Generating new fontManager, this may take some time...\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:04.861666: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:04.861694: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:06.396617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:06.396648: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:06.396671: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-154-143.us-west-2.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:06.396925: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:07,942 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:376678947624:processing-job/baseline-suggestion-job-2023-03-25-19-46-50-316', 'ProcessingJobName': 'baseline-suggestion-job-2023-03-25-19-46-50-316', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '159807026194.dkr.ecr.us-west-2.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-west-2-376678947624/telco-5g-observabiltiy/data/xgboost/train.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-west-2-376678947624/telco-5g-observabiltiy/monitoring/baselining/results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::376678947624:role/service-role/AmazonSageMaker-ExecutionRole-20220927T095532', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:07,942 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:07,942 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:07,942 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:07,942 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,000 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,000 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,001 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,007 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,007 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,008 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,462 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.154.143\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.\u001b[0m\n",
      "\u001b[34m0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_362\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,472 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,477 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-bd8a5501-ac14-4c50-a719-0a3a3772b996\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,987 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,998 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:08,999 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,001 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,006 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,006 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,006 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,006 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,037 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,048 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,048 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,051 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,055 INFO blockmanagement.BlockManager: The block deletion will start around 2023 Mar 25 19:51:09\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,056 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,056 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,057 INFO util.GSet: 2.0% max memory 3.1 GB = 63.8 MB\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,057 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,136 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,140 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,165 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,166 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,166 INFO util.GSet: 1.0% max memory 3.1 GB = 31.9 MB\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,166 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,168 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,168 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,168 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,168 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,172 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,176 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,176 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,176 INFO util.GSet: 0.25% max memory 3.1 GB = 8.0 MB\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,176 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,182 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,182 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,182 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,185 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,185 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,187 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,187 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,187 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 979.8 KB\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,187 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,208 INFO namenode.FSImage: Allocated new BlockPoolId: BP-454027294-10.0.154.143-1679773869202\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,219 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,226 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,299 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,311 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,314 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.154.143\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:09,324 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:11,383 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:11,383 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:13,459 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:13,459 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:15,565 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:15,565 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:17,672 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:17,673 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:19,786 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:19,787 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:29,792 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:31,446 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:31,843 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:31,877 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:31,890 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,324 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,347 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,347 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,347 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,348 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,370 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11544, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,383 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,385 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,431 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,432 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,432 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,433 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,433 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,790 INFO util.Utils: Successfully started service 'sparkDriver' on port 34355.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,825 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,870 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,899 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,899 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,943 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,975 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4dd1affd-abd0-4b34-8dcd-0a60414cce93\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:32,995 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:33,043 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:33,085 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.154.143:34355/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1679773892320\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:33,646 INFO client.RMProxy: Connecting to ResourceManager at /10.0.154.143:8032\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,439 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,439 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,446 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15743 MB per container)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,447 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,447 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,447 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,453 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:34,535 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:36,350 INFO yarn.Client: Uploading resource file:/tmp/spark-fb069340-87c9-43ec-a434-d9a1e9888be0/__spark_libs__1174358865297614976.zip -> hdfs://10.0.154.143/user/root/.sparkStaging/application_1679773875166_0001/__spark_libs__1174358865297614976.zip\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:37,969 INFO yarn.Client: Uploading resource file:/tmp/spark-fb069340-87c9-43ec-a434-d9a1e9888be0/__spark_conf__2197901701158396231.zip -> hdfs://10.0.154.143/user/root/.sparkStaging/application_1679773875166_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,031 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,031 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,031 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,031 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,031 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,063 INFO yarn.Client: Submitting application application_1679773875166_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:38,288 INFO impl.YarnClientImpl: Submitted application application_1679773875166_0001\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:39,295 INFO yarn.Client: Application report for application_1679773875166_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:39,299 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sat Mar 25 19:51:38 +0000 2023] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1679773898181\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1679773875166_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:40,302 INFO yarn.Client: Application report for application_1679773875166_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:41,308 INFO yarn.Client: Application report for application_1679773875166_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:42,310 INFO yarn.Client: Application report for application_1679773875166_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:43,314 INFO yarn.Client: Application report for application_1679773875166_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:43,912 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1679773875166_0001), /proxy/application_1679773875166_0001\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,317 INFO yarn.Client: Application report for application_1679773875166_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,321 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.154.143\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1679773898181\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1679773875166_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,323 INFO cluster.YarnClientSchedulerBackend: Application application_1679773875166_0001 has started running.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,340 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38165.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,341 INFO netty.NettyBlockTransferService: Server created on 10.0.154.143:38165\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,342 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,350 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.154.143, 38165, None)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,354 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.154.143:38165 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.154.143, 38165, None)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,360 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.154.143, 38165, None)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,361 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.154.143, 38165, None)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:44,533 INFO util.log: Logging initialized @14517ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:45,436 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:49,044 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.154.143:39458) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:51:49,203 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:35169 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 35169, None)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:03,543 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:03,716 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:03,774 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:03,778 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:04,872 INFO datasources.InMemoryFileIndex: It took 48 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,024 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,349 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,351 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.154.143:38165 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,356 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,707 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,709 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,715 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,773 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,792 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,793 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,793 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,795 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,802 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,849 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,858 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,859 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.154.143:38165 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,860 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,876 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,877 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:05,916 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4618 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:06,139 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:35169 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:06,874 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:35169 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,203 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1302 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,205 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,211 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.387 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,223 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,223 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,225 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.451700 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,445 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.154.143:38165 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,453 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:35169 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,475 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.154.143:38165 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:07,518 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:35169 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,554 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,556 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,560 INFO datasources.FileSourceStrategy: Output Data Schema: struct<anomaly: string, health: string, accessibility: string, 5g_users: string, contention_rate: string ... 6 more fields>\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,747 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,765 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,766 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.154.143:38165 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,769 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,784 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,829 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:100\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,831 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:100) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,831 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:100)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,831 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,834 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:09,842 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,004 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,006 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,007 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.154.143:38165 (size: 7.8 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,009 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,010 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:100) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,011 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,016 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,066 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:35169 (size: 7.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:10,833 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:35169 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,582 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:35169 (size: 3.7 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,691 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1679 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,692 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,695 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:100) finished in 1.850 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,696 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,696 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,696 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:100, took 1.866707 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:11,942 INFO codegen.CodeGenerator: Code generated in 187.250371 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,439 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,582 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,586 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,587 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,587 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,590 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,599 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,625 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 113.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,627 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,628 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.154.143:38165 (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,629 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,631 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,631 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,640 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:12,664 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:35169 (size: 34.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,550 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1912 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,550 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,552 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.948 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,553 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,553 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,554 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,554 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,642 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,644 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,645 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,646 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,649 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,650 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,666 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 166.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,668 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 45.9 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,669 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.154.143:38165 (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,670 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,670 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,670 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,673 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,699 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:35169 (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:14,749 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,174 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 502 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,174 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,175 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.517 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,175 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,175 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,176 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.533655 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,216 INFO codegen.CodeGenerator: Code generated in 29.65682 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,501 INFO codegen.CodeGenerator: Code generated in 38.519811 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,630 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,633 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,633 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,634 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,634 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,635 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,636 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.154.143:38165 in memory (size: 7.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,641 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:35169 in memory (size: 7.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,694 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 37.1 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,703 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:35169 in memory (size: 34.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,703 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,704 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.154.143:38165 in memory (size: 34.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,706 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.154.143:38165 (size: 16.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,707 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,708 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,708 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,709 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,745 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:35169 (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,779 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:35169 in memory (size: 45.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:15,783 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.154.143:38165 in memory (size: 45.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,478 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1769 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,478 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,479 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.839 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,479 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,480 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:17,480 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.848265 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,069 INFO codegen.CodeGenerator: Code generated in 118.812734 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,077 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,077 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,077 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,077 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,078 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,080 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,085 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 73.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,087 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 23.6 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,087 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.154.143:38165 (size: 23.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,088 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,089 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,089 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,090 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,109 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:35169 (size: 23.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,407 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 317 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,408 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,409 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.328 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,410 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,411 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,411 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,411 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,581 INFO codegen.CodeGenerator: Code generated in 97.349391 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,592 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,594 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,594 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,594 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,594 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,595 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,598 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 66.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,600 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,601 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.154.143:38165 (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,601 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,602 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,602 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,604 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,625 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:35169 (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,630 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,778 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 174 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,778 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,779 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.182 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,781 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,781 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,781 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.188906 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:18,923 INFO codegen.CodeGenerator: Code generated in 93.470067 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,080 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,084 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,084 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,084 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,084 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,084 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,087 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,097 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 29.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,099 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 13.6 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,100 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.154.143:38165 (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,101 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,101 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,102 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,103 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:19,126 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:35169 (size: 13.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,852 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1749 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,852 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.765 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,853 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,855 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,857 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,857 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.154.143:38165 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,858 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,858 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,858 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,860 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,874 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:35169 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,879 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,922 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,922 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,923 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,923 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,923 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:20,925 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.844773 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,111 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,111 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,111 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,111 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,112 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,113 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,120 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 62.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,123 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 22.2 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,123 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.154.143:38165 (size: 22.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,125 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,126 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,126 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,129 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,141 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:35169 (size: 22.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,656 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 527 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,656 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,657 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.544 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,658 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,658 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,658 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,658 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,688 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,689 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,689 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,689 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,690 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,691 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,695 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 114.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,696 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,697 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.154.143:38165 (size: 34.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,699 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,700 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,700 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,701 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,714 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:35169 (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,723 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,829 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 128 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,829 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,830 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.139 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,830 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,830 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,831 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.142830 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,843 INFO codegen.CodeGenerator: Code generated in 9.722732 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:21,961 INFO codegen.CodeGenerator: Code generated in 26.949111 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,023 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,029 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,029 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,029 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,030 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,031 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,034 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.154.143:38165 in memory (size: 19.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,035 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:35169 in memory (size: 19.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,061 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 35.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,063 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,063 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.154.143:38165 (size: 15.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,064 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,064 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,064 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,066 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4946 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,068 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:35169 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,072 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.154.143:38165 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,087 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:35169 (size: 15.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,117 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.154.143:38165 in memory (size: 34.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,121 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:35169 in memory (size: 34.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,208 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.154.143:38165 in memory (size: 23.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,209 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:35169 in memory (size: 23.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,248 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.154.143:38165 in memory (size: 13.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,252 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:35169 in memory (size: 13.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,262 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.154.143:38165 in memory (size: 16.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,265 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:35169 in memory (size: 16.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,284 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.154.143:38165 in memory (size: 22.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,293 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:35169 in memory (size: 22.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,996 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 929 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,997 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 0.962 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,998 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,998 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,999 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:22,999 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 0.972835 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,153 INFO codegen.CodeGenerator: Code generated in 39.862418 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,159 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,160 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,160 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,160 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,161 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,162 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,165 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 51.9 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,167 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,168 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.154.143:38165 (size: 18.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,168 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,169 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,169 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,171 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,182 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:35169 (size: 18.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,457 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 286 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,458 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,458 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.295 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,459 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,459 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,459 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,460 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,534 INFO codegen.CodeGenerator: Code generated in 37.719085 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,542 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,544 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,544 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,544 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,545 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,545 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,547 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 43.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,549 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,550 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.154.143:38165 (size: 14.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,550 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,551 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,551 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,553 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,568 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:35169 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,572 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,632 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,632 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,633 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.087 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,635 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,636 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,636 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.093718 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,685 INFO codegen.CodeGenerator: Code generated in 39.979787 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,931 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\n",
      "\u001b[34m2023-03-25 19:52:23,967 INFO codegen.CodeGenerator: Code generated in 9.727961 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,975 INFO scheduler.DAGScheduler: Registering RDD 77 (count at StatsGenerator.scala:66) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,975 INFO scheduler.DAGScheduler: Got map stage job 13 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,975 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,975 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,976 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,976 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[77] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,979 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 21.4 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,981 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,981 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.154.143:38165 (size: 10.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,982 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,982 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[77] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,982 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,984 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4935 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:23,996 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:35169 (size: 10.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,038 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 55 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,038 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,039 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (count at StatsGenerator.scala:66) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,040 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,040 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,041 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,041 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,059 INFO codegen.CodeGenerator: Code generated in 8.620765 ms\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,068 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,070 INFO scheduler.DAGScheduler: Got job 14 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,070 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,070 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,070 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,070 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[80] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,072 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 11.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,073 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,073 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.154.143:38165 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,074 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,075 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[80] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,075 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,076 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,087 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:35169 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,091 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.154.143:39458\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,105 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 15) in 29 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,106 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,107 INFO scheduler.DAGScheduler: ResultStage 20 (count at StatsGenerator.scala:66) finished in 0.035 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,107 INFO scheduler.DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,107 INFO cluster.YarnScheduler: Killing all running tasks in stage 20: Stage finished\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,107 INFO scheduler.DAGScheduler: Job 14 finished: count at StatsGenerator.scala:66, took 0.038526 s\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,579 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,597 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,654 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,655 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,662 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,689 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,738 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,739 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,752 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,756 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,810 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,810 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,810 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,833 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,834 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-fb069340-87c9-43ec-a434-d9a1e9888be0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,849 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d4ccfde0-eeac-4497-970a-dfefb7e4f489\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,941 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2023-03-25 19:52:24,941 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f484c91a750>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "# this is our training dataset\n",
    "baseline_data_uri = train_s3_path\n",
    "baseline_results_prefix = f\"{prefix}/monitoring/baselining/results\"\n",
    "baseline_results_uri = f\"s3://{bucket}/{baseline_results_prefix}\"\n",
    "\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2d41b-fadb-4fdb-a696-59069781dbdc",
   "metadata": {},
   "source": [
    "Baseline process generates a contraints and statistics configuration files, we are going to preview the generated json files. Keep in mind, you can also supply your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23d23c2d-e122-4f5a-a2c1-ab655d619b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Files:\n",
      "telco-5g-observabiltiy/monitoring/baselining/results/constraints.json\n",
      " telco-5g-observabiltiy/monitoring/baselining/results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "result = s3_client.list_objects(Bucket=bucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get(\"Contents\")]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3bd4a-541b-48f1-9c96-16b55553de47",
   "metadata": {},
   "source": [
    "**Statistics** refer to the expected statistical properties of the input data, such as mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2bdd6d7d-6375-4ae7-9ff3-5436ed22dad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>Integral</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.146301</td>\n",
       "      <td>9898.000000</td>\n",
       "      <td>0.353408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966063</td>\n",
       "      <td>65358.960000</td>\n",
       "      <td>0.110403</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.5, 'upper_bound': 0.55, 'co...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accessibility</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.872957</td>\n",
       "      <td>59059.900000</td>\n",
       "      <td>0.319040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5g_users</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>2311.429715</td>\n",
       "      <td>0.055447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0028818443804034, 0.0089657380723663, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contention_rate</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>215.332653</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.00022675736961451248, 0.0, 0.0, 0.00022675...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>utilization</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251875</td>\n",
       "      <td>17040.611878</td>\n",
       "      <td>0.109768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.1408839779005525, 0.281767955801105, 0.151...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>downlink_throughput</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>1399.434372</td>\n",
       "      <td>0.045512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0001264799562459171, 0.0017395406966993, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uplink_throughput</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>67655</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027495</td>\n",
       "      <td>1860.168126</td>\n",
       "      <td>0.045460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0009849585496016764, 0.0100781843310955, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0              anomaly      Integral                                    67655   \n",
       "1               health    Fractional                                    67655   \n",
       "2        accessibility    Fractional                                    67655   \n",
       "3             5g_users    Fractional                                    67655   \n",
       "4      contention_rate    Fractional                                    67655   \n",
       "5          utilization    Fractional                                    67655   \n",
       "6  downlink_throughput    Fractional                                    67655   \n",
       "7    uplink_throughput    Fractional                                    67655   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.146301   \n",
       "1                                        0                   0.966063   \n",
       "2                                        0                   0.872957   \n",
       "3                                        0                   0.034165   \n",
       "4                                        0                   0.003183   \n",
       "5                                        0                   0.251875   \n",
       "6                                        0                   0.020685   \n",
       "7                                        0                   0.027495   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0               9898.000000                      0.353408   \n",
       "1              65358.960000                      0.110403   \n",
       "2              59059.900000                      0.319040   \n",
       "3               2311.429715                      0.055447   \n",
       "4                215.332653                      0.012855   \n",
       "5              17040.611878                      0.109768   \n",
       "6               1399.434372                      0.045512   \n",
       "7               1860.168126                      0.045460   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       0.5                       1.0   \n",
       "2                       0.0                       1.0   \n",
       "3                       0.0                       1.0   \n",
       "4                       0.0                       1.0   \n",
       "5                       0.0                       1.0   \n",
       "6                       0.0                       1.0   \n",
       "7                       0.0                       1.0   \n",
       "\n",
       "       numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "1  [{'lower_bound': 0.5, 'upper_bound': 0.55, 'co...   \n",
       "2  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "3  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "4  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "5  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "6  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "7  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'cou...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                               0.64           \n",
       "1                                               0.64           \n",
       "2                                               0.64           \n",
       "3                                               0.64           \n",
       "4                                               0.64           \n",
       "5                                               0.64           \n",
       "6                                               0.64           \n",
       "7                                               0.64           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                             2048.0           \n",
       "1                                             2048.0           \n",
       "2                                             2048.0           \n",
       "3                                             2048.0           \n",
       "4                                             2048.0           \n",
       "5                                             2048.0           \n",
       "6                                             2048.0           \n",
       "7                                             2048.0           \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "1  [[1.0, 0.93, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0...  \n",
       "2  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "3  [[0.0028818443804034, 0.0089657380723663, 0.00...  \n",
       "4  [[0.00022675736961451248, 0.0, 0.0, 0.00022675...  \n",
       "5  [[0.1408839779005525, 0.281767955801105, 0.151...  \n",
       "6  [[0.0001264799562459171, 0.0017395406966993, 0...  \n",
       "7  [[0.0009849585496016764, 0.0100781843310955, 0...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ce650-eda1-4fc5-b11c-b80287138bda",
   "metadata": {},
   "source": [
    "**Constraints** are rules that are used to ensure that the model's performance does not degrade beyond a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ff450b7-5ae3-494a-9e25-c2999ca23bee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anomaly</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>health</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>accessibility</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5g_users</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contention_rate</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>utilization</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>downlink_throughput</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>uplink_throughput</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name inferred_type  completeness  \\\n",
       "0              anomaly      Integral           1.0   \n",
       "1               health    Fractional           1.0   \n",
       "2        accessibility    Fractional           1.0   \n",
       "3             5g_users    Fractional           1.0   \n",
       "4      contention_rate    Fractional           1.0   \n",
       "5          utilization    Fractional           1.0   \n",
       "6  downlink_throughput    Fractional           1.0   \n",
       "7    uplink_throughput    Fractional           1.0   \n",
       "\n",
       "   num_constraints.is_non_negative  \n",
       "0                             True  \n",
       "1                             True  \n",
       "2                             True  \n",
       "3                             True  \n",
       "4                             True  \n",
       "5                             True  \n",
       "6                             True  \n",
       "7                             True  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraints_df = pd.io.json.json_normalize(\n",
    "    baseline_job.suggested_constraints().body_dict[\"features\"]\n",
    ")\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b94347-8a5d-427a-a750-d365284c1c00",
   "metadata": {},
   "source": [
    "#### 2. Create a schedule to analyze collected data for data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "092a6a67-d0c3-4826-9c80-9d51e7c75e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The endpoint attribute has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: telco-5g-observabiltiy-monitoring-job-1679773970-94ce\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "\n",
    "mon_schedule_name = unique_name_from_base(f\"{prefix}-monitoring-job\")\n",
    "\n",
    "s3_report_path = f\"s3://{bucket}/{prefix}/montoring/report\"\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint, #predictor endpoint name\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(), #Hourly\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bbf953b-1edc-4196-83b4-23a8e702c036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Pending\n"
     ]
    }
   ],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print(\"Schedule status: {}\".format(desc_schedule_result[\"MonitoringScheduleStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2732643a-21c5-4320-8624-eb53c09bd21e",
   "metadata": {},
   "source": [
    "### Generate some artificial traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7568fa71-2f23-425c-8d4a-d39b6f8c2be9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting artificial traffic batch 10 ...\n",
      "predicting artificial traffic batch 20 ...\n",
      "predicting artificial traffic batch 30 ...\n",
      "predicting artificial traffic batch 40 ...\n",
      "predicting artificial traffic batch 50 ...\n",
      "predicting artificial traffic batch 60 ...\n",
      "predicting artificial traffic batch 70 ...\n",
      "predicting artificial traffic batch 80 ...\n",
      "predicting artificial traffic batch 90 ...\n",
      "predicting artificial traffic batch 100 ...\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(100):\n",
    "\n",
    "    predict(test.to_numpy()[:, 1:])\n",
    "    count+=1\n",
    "    if count%10 == 0:\n",
    "        print(f\"predicting artificial traffic batch {count} ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4cafc7-293f-4b80-81b2-5f6cb2a1fef1",
   "metadata": {},
   "source": [
    "Depend on the cron job you defined, you may need to wait a bit for your monitor job to execute.  Once it does, the execution will be listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "875cb453-6662-40b5-8d92-eb09e4ee5690",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No executions found for schedule. monitoring_schedule_name: telco-5g-observabiltiy-monitoring-job-1679773970-94ce\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "mon_executions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbce37-004f-4dcd-80ee-edfc37992e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df94c6d-26ef-430d-a5ab-29c3c583ee1d",
   "metadata": {},
   "source": [
    "Delete feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6330e71e-f238-4127-a0cf-4e82d9dea7d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7abc626a-9c77-4875-9798-1c263d759681',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7abc626a-9c77-4875-9798-1c263d759681',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Sat, 25 Mar 2023 19:54:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group_name = fg_name\n",
    "sagemaker_client.delete_feature_group(\n",
    "    FeatureGroupName= feature_group_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0665e257-bc91-4ae2-bb5f-491fabc05c7a",
   "metadata": {},
   "source": [
    "Remove experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18e60355-6025-4d6f-881f-8082a855561a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrialNames:\n",
      "\n",
      "Default-Run-Group-telco-5g-observabiltiy-1679772647-d8f7\n",
      "\tTrialComponentNames:\n",
      "\tsagemaker-scikit-learn-2023-03-25-19-39-53-333-aws-training-job\n",
      "\ttelco-5g-observabiltiy-1679772647-d8f7-isoforest-experiment-1679773192-70a6\n",
      "\tsagemaker-xgboost-2023-03-25-19-30-48-305-aws-training-job\n",
      "\ttelco-5g-observabiltiy-1679772647-d8f7-xgboost-experiment-1679772647-ca49\n",
      "\n",
      "Experiment telco-5g-observabiltiy-1679772647-d8f7 deleted\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def remove_experiment(experiment_name):\n",
    "    trials = sagemaker_client.list_trials(ExperimentName=experiment_name)['TrialSummaries']\n",
    "    print('TrialNames:')\n",
    "    for trial in trials:\n",
    "        trial_name = trial['TrialName']\n",
    "        print(f\"\\n{trial_name}\")\n",
    "\n",
    "        components_in_trial = sagemaker_client.list_trial_components(TrialName=trial_name)\n",
    "        print('\\tTrialComponentNames:')\n",
    "        for component in components_in_trial['TrialComponentSummaries']:\n",
    "            component_name = component['TrialComponentName']\n",
    "            print(f\"\\t{component_name}\")\n",
    "            sagemaker_client.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                sagemaker_client.delete_trial_component(TrialComponentName=component_name)\n",
    "            except:\n",
    "                # component is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(.5)\n",
    "        sagemaker_client.delete_trial(TrialName=trial_name)\n",
    "    sagemaker_client.delete_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "\n",
    "remove_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b7c166-7189-4407-acdf-f4501747bf6a",
   "metadata": {},
   "source": [
    "Remove Model Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0febe27-c8fa-4433-a870-8080870aaa99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Deleting Monitoring Schedule with name: telco-5g-observabiltiy-monitoring-job-1679773970-94ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Deleting Data Quality Job Definition with name: data-quality-job-definition-2023-03-25-19-52-50-993\n"
     ]
    }
   ],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2f897b-4811-4ebb-aa60-da0e44b5281f",
   "metadata": {},
   "source": [
    "Remove endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c069a782-6683-4213-81dd-76b0005c0c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring Schedule:\n",
      "Endpoint sagemaker-xgboost-2023-03-25-19-33-45-597 deleted\n",
      "Monitoring Schedule:\n",
      "Endpoint sagemaker-scikit-learn-2023-03-25-19-42-47-663 deleted\n"
     ]
    }
   ],
   "source": [
    "def remove_endpoint(endpoint_name):\n",
    "    monitor_schedules = sagemaker_client.list_monitoring_schedules(EndpointName=endpoint_name)['MonitoringScheduleSummaries']\n",
    "    print('Monitoring Schedule:')\n",
    "    for ms in monitor_schedules:\n",
    "        ms_name = ms['MonitoringScheduleName']\n",
    "        print(f\"\\n{ms_name}\")\n",
    "\n",
    "        sagemaker_client.delete_monitoring_schedule(MonitoringScheduleName=ms_name)\n",
    "        \n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Endpoint {endpoint_name} deleted\")\n",
    "\n",
    "#xgboost\n",
    "remove_endpoint(predictor.endpoint_name)\n",
    "# #isolation forest\n",
    "remove_endpoint(isoforest_predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ffc53-9d22-49bf-bbca-39591c1c4e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
